from typing import Dict, Any, List, Optional, Annotated, get_args
from fastapi import APIRouter, HTTPException, UploadFile, File, Form


from .models import ChatMessageInput, ChatMessageOutput, FileInformation
from ..utils.ai_handler import AIInteractionHandler
from ..utils.medical_memory import MedicalMemory, ConversationMode

# from ..config import settings # Not directly needed here if AIHandler uses it

router = APIRouter()

ai_handler = AIInteractionHandler()
memory_handler = MedicalMemory() # Single persistent user, but manages modes internally

@router.post("/chat", response_model=ChatMessageOutput)
async def handle_chat_message(
    message: Annotated[Optional[str], Form()] = None,
    mode_str: Annotated[str, Form()] = "qna", # Receive mode as string
    user_region: Annotated[Optional[str], Form()] = None,
    upload_file: Annotated[Optional[UploadFile], File()] = None
):
    current_mode: ConversationMode
    if mode_str not in get_args(ConversationMode): # Validate against Literal values
        raise HTTPException(status_code=400, detail=f"Invalid mode: '{mode_str}'. Must be one of {get_args(ConversationMode)}.")
    current_mode = mode_str # type: ignore

    file_info_model: Optional[FileInformation] = None
    input_message = message # User's text message
    
    if upload_file:
        if upload_file.size > 10 * 1024 * 1024: # 10MB limit
            raise HTTPException(status_code=413, detail="File too large. Max 10MB.")
        
        contents_bytes = await upload_file.read()
        import base64
        content_base64 = base64.b64encode(contents_bytes).decode('utf-8')
        
        file_info_model = FileInformation(
            name=upload_file.filename or "uploaded_file",
            type=upload_file.content_type or "application/octet-stream",
            size=len(contents_bytes), # Use actual byte length
            content_base64=content_base64
        )
        await upload_file.close()
        print(f"File received: {file_info_model.name}, Type: {file_info_model.type}, Size: {file_info_model.size}")
        if not input_message: # If only file is uploaded, make a default message for context
            input_message = f"Please analyze the uploaded file: {file_info_model.name}"

    if not input_message and not file_info_model:
        raise HTTPException(status_code=400, detail="No message or file provided.")

    # Use Pydantic model for consistent input to AI handler, built from Form data
    # This isn't strictly necessary here as we pass individual args, but good practice if AI handler expects a model
    # For now, we'll pass individual args as AI handler methods are defined that way
    file_info_model_dict = file_info_model.model_dump() if file_info_model else None

    history_context = memory_handler.get_context_for_ai(current_mode)
    response_data_dict: Dict[str, Any] = {}

    try:
        if current_mode == "qna":
            response_data_dict = await ai_handler.get_general_qna_answer(
                input_message or "User uploaded a file for context. Please see file details if relevant.", # Ensure some message
                history_context, 
                file_info_model_dict
            )
        elif current_mode == "personal_symptoms":
            if not input_message: 
                raise HTTPException(status_code=400, detail="Symptom description is required for this mode.")
            response_data_dict = await ai_handler.analyze_personal_symptoms(
                input_message, history_context, user_region
            )
        elif current_mode == "personal_report_upload":
            if not file_info_model_dict: 
                raise HTTPException(status_code=400, detail="File upload is required for report analysis mode.")
            response_data_dict = await ai_handler.analyze_uploaded_personal_report(
                file_info_model_dict, history_context, user_region
            )
        # No 'else' needed due to mode_str validation earlier

        # Ensure all fields for ChatMessageOutput are present, defaulting if necessary
        # The _parse_ai_response_to_structured_output in ai_handler should mostly handle this.
        # This is a final safety net.
        final_output_data = {
            "answer": response_data_dict.get("answer", "No specific answer generated by AI."),
            "answer_format": response_data_dict.get("answer_format", "markdown"),
            "follow_up_questions": response_data_dict.get("follow_up_questions"),
            "disease_identification": response_data_dict.get("disease_identification"),
            "next_steps": response_data_dict.get("next_steps"),
            "government_schemes": response_data_dict.get("government_schemes"),
            "doctor_recommendations": response_data_dict.get("doctor_recommendations"),
            "graphs_data": response_data_dict.get("graphs_data"),
            "error": response_data_dict.get("error"),
            "file_processed_with_message": response_data_dict.get("file_processed_with_message")
        }
        response_output = ChatMessageOutput(**final_output_data)

        # Save to conversation history for the specific mode
        memory_handler.add_to_conversation_history(
            mode=current_mode,
            user_message=input_message, 
            ai_response=response_output.answer, # Storing main answer text
            # To store the full AI JSON response for richer history display:
            # ai_response_full_obj=response_output.model_dump_json(), # Store full ChatMessageOutput as JSON string
            file_name=file_info_model.name if file_info_model else None
        )
        
        if current_mode in ["personal_symptoms", "personal_report_upload"] and response_data_dict.get("extracted_medical_info"):
            memory_handler.update_medical_summary(response_data_dict["extracted_medical_info"])
            
        return response_output

    except HTTPException as e:
        raise e # Re-raise HTTPExceptions from validation or AI handler
    except Exception as e:
        print(f"Critical Error in /chat endpoint processing mode '{current_mode}': {e.__class__.__name__} - {str(e)}")
        import traceback
        traceback.print_exc()
        return ChatMessageOutput(answer=f"Sorry, an unexpected server error occurred while processing your request for {current_mode}.", error=str(e))


@router.get("/history/{mode_str}", response_model=List[Dict[str, Any]])
async def get_mode_history_route(mode_str: str):
    current_mode: ConversationMode
    if mode_str not in get_args(ConversationMode):
        raise HTTPException(status_code=400, detail=f"Invalid mode for history: '{mode_str}'.")
    current_mode = mode_str #type: ignore
    return memory_handler.get_conversation_history(current_mode)

@router.get("/history/summary/all")
async def get_all_history_summary_route(): # Renamed to avoid conflict if class has same name
    conv_summary = memory_handler.get_all_conversations_summary()
    med_summary = memory_handler.get_medical_summary()
    return {
        "conversation_summaries": conv_summary,
        "medical_summary": med_summary
    }

@router.post("/history/clear/all")
async def clear_all_data_route(): # Renamed
    memory_handler.clear_all_user_data()
    return {"message": "All user data has been cleared."}